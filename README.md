# UC BERKELEY - MACHINE LEARNING & AI CAPSTONE PROJECT

<h2 align='center'> <a href="https://github.com/kfmatovic716/HEALTH-INSURANCE-COST-ML.git">HEALTH INSURANCE COST PREDICTION & RISK ANALYSIS USING MACHINE LEARNING TECHNIQUES</a></h2>

![Logo](images/healthinsurance.jpeg)


## INTRODUCTION
The growing emphasis of health equity has recently become a major priority in the United States and it has contributed to the rising healthcare costs. Leveraging health insurance data to predict future claims costs and identifying key factors that drive these  escalating costs can provide actionable insights to improve decision-making and expense management of health insurance providers.

## PROBLEM STATEMENTS 
<ul>
    <li>Based on an individual’s demographic and lifestyle, can we predict the individual’s health insurance charges accurately?</li>
    <li>Can we classify who would likely incur <strong>high</strong> versus <strong>low</strong> health insurance costs?</li>
    <li>What factors strongly influence health insurance costs? </li>
</ul>

## GOALS 
<ul>
    <li><strong>Cost Prediction</strong> - Build a baseline predictive model using <strong>Linear Regression</strong>, then enhance performance using <strong>Ridge and Lasso Regression</strong>. Evaluate models using <strong>R-squared, Mean Absolute Error (MAE), and Mean Squared Error (MSE)</strong> metrics.</li>
    <li><strong>Risk Classification</strong> - Classify individuals into <strong>high-cost or low-cost</strong> categories using Logistic Regression, based on lifestyle features and/or predicted charges.
</li>
    <li><strong>Feature Importance</strong> - Determine the most influential feature that affects health insurance cost using <strong>GridSearchCV</strong></li>
</ul>


## DATA ACQUISITION
<ul>
    <li><strong>SOURCE: </strong> Kaggle ➡️ <a href="https://www.kaggle.com/datasets/harishkumardatalab/medical-insurance-price-prediction"><strong style="font-size: 16px;">Health Insurance Cost Data</strong></a></li>
    <li><strong>DATA SIZE: </strong>The raw data has 7 features and 2,772 records in total</li>
</ul>

## DATA DESCRIPTIONS
<p>The dataset contains the following features, along with their data types and corresponding descriptions: </p>
<ul>
    <li>age - person’s age</li>
    <li>sex - person’s gender</li>
    <li>bmi -  body mass index</li>
    <li>children - number of children</li>
    <li>smoker - indicates if person is smoker (“yes”) or non-smoker (“no”)</li>
    <li>region -  US region where person resides</li>
    <li>charges (target variable) - insurance premium price</li>
</ul>

## DATA TRANSFORMATIONS
<p>Utilized the map function by converting  all categorical variables (sex, region, and smoker)  into a numeric form. This allows them to be included in the correlation matrix to explore relationships with other features and also supports the modeling stage by ensuring compatibility with running machine learning algorithms.
</p>

## EXPLORATORY DATA ANALYSIS (EDA)
<ul>
    <li><strong>Missing Values and Duplicate Records</strong></li>
    <ul>
        <li>There were no missing values in the column features</li>
        <li>There were 1,435 duplicate rows that were deleted from the rawdata; Some of the records didnt just have one duplicate but 3 duplicates!</li>
        <li>There were <strong>total of 1,337 records in the new dataset</strong> after deleting duplicates</li>
    </ul>
    <br>
    <li><strong>Univariate Analysis and Outlier Detection</strong></li>
    <ul>
        <img src="/images/age.png"/>
        <li>The age distribution is skewed to the right, which means that the majority of individuals fall within the younger age range (18–20). The curve then stretches gradually toward older ages, extending up to age 63. The smallest representation is seen at ages 64–65, which aligns with expectations in the context of insurance: individuals over 65 are typically considered higher risk, making them less desirable participants for many insurance plans. </li>
        <li>Age appears to have a consistent distribution and no present outliers in the feature</li>
        <img src="/images/bmi.png"/>
        <li>Body Mass Index categories are as follows: underweight are <18.5, healthy weight: 18.5 to less than 25, overweight: 25.0 to less than 30.0 and obesity: >=30.0 </li>
        <li>The BMI illustrates an approximately normal distribution, with the highest concentration of values between 25 and 35 which corresponds to the overweight and obese categories. The tail-end distributions represent individuals in the healthy weight range on the lower end and the severely obese group on the upper end. </li>
        <li>This distribution reflects prevailing health and lifestyle patterns in the US. A significant proportion of the population is overweight, largely attributed to poor nutrition, high stress levels, and sedentary lifestyles.</li>
        <li>There were a few outliers (approximately 7) with BMI over 47 that are severely obese </li>
        <img src="/images/children.png"/>
        <li>The 'number of children’ feature shows a right-skewed distribution. The majority of participants (approximately 575) reported having no children, followed by 324 participants with one child. The frequency declines progressively as the number of children increases, forming a long tail toward higher values. </li>
        <li>There were no outliers to be observed in this feature</li>
        <img src="/images/charges.png"/>
        <li>The health insurance charges, which serves as the target variable, is right-skewed, indicating that the majority of the population pays less than $14K. However, there is a notable presence of outliers, with some individuals incurring costs exceeding $34K.</li>
        <li>The dataset have participants who have high premium charges, which are considered outliers. It is important to keep them because they usually reflect real people with unusual but valid health/risk profiles — e.g., older policyholders, smokers, or those with chronic conditions. A biased model may result and distort the true distribution of costs when excluded.</li> 
        <li>Outliers are critical to model generalization especially with a classification task that identifies policyholders who are low and high risk. Eliminating these records could make the model less effective in recognizing the most important class. Outliers play a crucial role in business decision making, particularly since in this dataset they mostly represent high-premium policyholders (with costs above $34K). These cases not only influence premium pricing strategies but also help insurers anticipate and prepare for costly claims. </li>    
        <img src="/images/gender.png"/>
        <li>Population in gender feature is evenly distributed, approximately 50% are  males and 50% female participants, which reduces bias in model training.</li>
        <li>Regulators emphasize non-discriminatory models especially in sensitive domains like healthcare and insurance. A balanced distribution meets this ethical consideration and fairness requirement.</li>     
        <img src="/images/smoker.png"/>
        <li>There is approximately 80% non-smoker population in this dataset, indicating class imbalance. </li>
        <li>The model might achieve high accuracy by mostly predicting “non-smoker,” but it would perform poorly in detecting smokers without adjustment</li>
        <img src="/images/region.png"/>
        <li>The regions of origin are evenly distributed among participants in this dataset, with the exception of the Southeast, which has a slightly higher representation</li>
    </ul>
    <br>
    <li><strong>Bivariate Analysis</strong></li>
        <ul>
            <img src="/images/corrmatrix.png"/>
            <li>Based on the figure above, the top 3 features that have positive relationship and directly impacts healthcare insurance premium charges are: smoker (~80% correlation), age (~30%) and BMI (~20%) </li>
            <li>Having children, gender and the region a policyholder is coming from are features that does not directly impact healthcare insurace charges.</li> 
            <img src="/images/smokervscharges.png"/>
            <li>The feature most strongly associated with health insurance costs is smoking status of a policyholder. This aligns with real-world evidence, as smokers face a significantly higher risk of developing life-threatening conditions such as cancer, stroke, and diabetes. As a result, they are classified as high-risk policyholders, making smoking status a critical factor in predicting insurance premiums.</li> 
            <li>The figure shows that smokers exhibit much greater variability in insurance premium costs, with charges reaching up to $60K, compared to non-smokers whose costs peak around $20K. The difference is significant, with smokers having a median cost of approximately $34K versus just $7K for non-smokers. </li>
            <li>The non-smokers have some outliers that are not much of a significant concern because they are still lower than the typical cost for smokers</li> 
            <img src="/images/agevscharges.png"/>
            <li>The red regression line illustrates a positive correlation between the insurance charges and the policyholder's age and insurance tend to increase as the policyholder gets older.</li> 
            <li>At every age level, insurance charges vary widely, ranging from about $1,000 to $60,000. These variations are likely driven by additional factors such as smoking status, chronic health conditions, and BMI. In general, younger policyholders (age < 35) have lower charges but some charges were high mainly due to some outliers, who might be a smoker, has high BMI or has chronic disease. </li> 
            <li>Although the trendline shows a steady increase with age, cost variations remain large among older policyholders (age > 50), indicating that not all older individuals pay high insurance premiums. </li>     
            <img src="/images/bmivscharges.png"/>
             <li>The regression line shows an upward trend and a shallow slope. This suggests that higher BMI is associated with higher insurance charges but does not appear to be a strong predictor of charges due to the shallow slope.</li> 
            <li>Just like the relationship between age and insurance charges above, insurance charges vary widely at every BMI value, ranging from approximately $1,000 to over $60,000. It is likely that other factors such as smoker status, age and chronic diseases play a much significant role in determining costs.</li>
            <li>Most policyholders fall in between approximately 20 to 35, where charges vary significantly. This suggests that showing that BMI is not a reliable standalone predictor of charges.</li> 
            <li>Higher charges above $40K appear more frequently in BMI above 30. On the other hand, some with very high BMI are still in low charges. This again indicates that there are other factors that interacts with BMI in predicting charges.</li>
        </ul>
       <br>
    <li><strong>Multivariate Analysis</strong></li>
        <ul>
            <img src="/images/pairplot.png"/>
            <li>Across all ages, smokers consistenly incur higher charges than non-smokers. In general, older payholders pay more charges but with smoking status creates a significant difference than age alone.</li>
            <li>Smokers with high BMI over 30 incur high charges (some over $60K). Non-smokers show less of a clear pattern since many with high BMI still fall in the low-to-mid insuarnce charge range. This means that smoking amplifies the effect of BMI on costs.</li> 
            <li>There is no clear trend between age and BMI. In this dataset, age and BMI are independent. Smokers and non-smokers are scattered similarly across the BMI and age spectrum.</li>
            <li>Smoking status is the strongest, most consistent driver of high insurance charges across all features. In almost every subplot, smokers (yellow) are shifted charges upwars compared to non-smokers which reinforces the idea that smoking status is a key driver in predicting insurance costs, more influential than age or BMI.</li>
        </ul>
</ul>
            
## MODELLING USING MACHINE LEARNING ALGORITHMS

### BASELINE MODEL: LINEAR REGRESSION (Predicting Health Insurance Charges)
### Coefficients
<ul>
    <img src="/images/coef_results.png" width="200" height="300"/>
<br>    
    <li>Each coefficient is the expected dollar change in charges for a +1 unit increase in that feature, holding others constant. It shows how the prediction changes if you change just that one feature, while pretending everything else in the model stays the same. For example, when two people with the same features like age, gender, bmi, and number of children but one is a smoker and the other person is not, the model would predict a premium for the smoker to be $23k higher. </li>
    <li>The intercept is not much meaningful to predict baseline charges when other features zero. In this insurance dataset, values of features like age, BMI and smoking status are very important in predicting health insurance charges. </li>
<br>
</ul>

### Evaluation Metrics
<ul>
    <img src="/images/metrics_results.png" width="300" height="400"/>
<br>
    <li>The model illustrates approximately 81% of the variation in insurance charges across individuals. The  differences in charges between policyholders can be accounted for by the features such as age, sex, BMI, children, and smoking status.</li>
    <li>Predictions on average are off by approximately $6k (RMSE test). For example if the true insurance charge for a policyholder is $4,000, the model will predict it as $10k. The model usually predicts within about $4k (MAE test) of the true value. Occasionally it misses by much more (like $10k+), which nudges the RMSE up to $6k. Since RMSE isn’t significantly different with MAE, those bigger errors are present but not overwhelming.  </li>
</ul>

### RIDGE AND LASSO REGRESSION
### Coefficients
<img src="/images/coefs_ridge_lasso.png" width="400" height="500"/>
<ul>
    <li>The coefficients are identical to the baseline model but slightly lower especially with the smoker status. This results into a lower predicted premium compared to the baseline</li>
    <li>In between Ridge and Lasso, Lasso appears to have an even lower coefficients, which will result into the lowest predicted premium compared to Ridge. </li>
</ul>

### Evaluation Metrics
<img src="/images/metrics_ridge_lasso.png" width="400" height="500"/>
<ul>
    <li>Based on the R2 score, there is no significant difference in overall fit. Both model explains approximately 73% of training variance ad 80% test variance, which is an excellent consistency.</li>
    <li>With Ridge having a higher R2 score on the test set (0.806 vs 0.802), it explains that Ridge can generalize new data slightly better than Lasso.</li>
    <li>Ridge has lower RMSE and MAE on both training and test sets, which indicated that predictions are closer to actual values</li>
    <li>In terms of regularization behavior, Ridge keeps all features but penalizes large coefficients, which indicated good multicollinearity. In contrast, Lasso can drive some coefficients to zero as seen on the coefficients results, where sex=0. Lasso is great for feature selection and interpretability but not the most optimal for prediction due to higher error and worst accuracy.</li>
</ul>

### PREDICTION MODEL SUMMARY RESULTS
<img src="/images/predict_models.png" width="600" height="500"/>
<p>The linear regression model is the best performing model due to its high predictive accuracy based on R2 test (0.8068), which explains the most variance in unseen data and yields the lowest prediction errors on average (RMSE & MAE). Ridge and Lasso does not improve performace to justify the regularization penalty. If the goal were to improve model stability and good multicollinearity, Ridge would be a solid choice but in terms of pure performance, Linear is the best model.  </p>

### CLASSIFICATION MODEL - LOGISTIC REGRESSION

<ul>
    <li>Based on the acccuracy score of 98.1% on test set and 95.9% on training set, the overall model performance is strong with excellent feature generalization to new and unseen data, no indication of overfitting and very low MSE errors (0.041 training vs 0.019 test), which means that the model makes a stable and consistent predections.</li>
    
<img src="/images/conf_matrix.png" width="500" height="500"/>
    <li>Classification report shows class 0 as low-risk and class 1 as high-risk. With recall=1, the model does not misclassify high-risk patients, which is critical for determining premium costs and this can also be beneficial for other healthcare use cases. There were no false negatives so the model correctly identifies all high-risk patients.</li>
    <li>The high-risk precision of 0.92 is slightly lower but still acceptable. It just means that a few low-risk patients were flagged as high-risk, which is safer than not being able to identify high-risk cases.</li>
    
<img src="/images/roc_curve.png" width="500" height="500"/>
    <li>The ROC curve above shows a blue line that extends sharply toward the top-left corner, which indicates a high true positive rate (recall) even at a low false positive rate. This means the model can accurately distinguish between the two classes (low-risk and high-risk patients) with low chances of misclassification. </li>
    <li>The AUC (Area Under the Curve) = 1.0 further confirms that the model achieves perfect discrimination, successfully identifying all positive (high-risk) and negative (low-risk) cases without error.</li>

<img src="/images/test_25.png" width="800" height="500"/>
    <li>Re-ran the model with 25% test size to compare results. There were no significant changes in the recall sensitivity and precision. As illustrated in the confusion matrix above, no false negatives on high-risk patients were missed, model correctly identified high-risk patients (true positives). </li>
    <li> The precision was slightly lower due to the 10 false positives (low-risk patients who were incorrectly classified as high-risk) However, as noted above, this is acceptable given that the model successfully identified all high-risk cases. </li>
    <li>The AUC changed to 0.99 and this makes the model more realistic compared to a perfect AUC of 1.00, which can sometimes mean that it is overfitting.</li>    
    <li>This model with 25% test size has a more favorable outcome in healthcare-related risk prediction because it maintains an excellent predictive power while showing slightly more realistic performance. It still captures all true high-risk patients, with only a small trade-off in false positives.</li>
</ul>

### RECOMMENDATIONS ON BUSINESS APPLICATION

<p> </p>

### FUTURE CONSIDERATIONS ON DATA QUALITY           
<ul>
    <li>Multiple duplicate records were identified in the data cleaning phase and have been removed from the dataset. To improve efficiency and reduce data processing time, such duplicate entries should not have been included in the data collection or reporting process.</li>
    <li>Data quantity and quality are crucial in machine learning. Machine learning models “learn” by detecting patterns between features (inputs) and the target (output), which all originate from the dataset. The more samples that are high-quality and diverse given to the model, the more realistic, accurate and generalizable the predictions become. Complex models like logistic regression definitely need more data to create a reliable and stable model. The risk of overfitting can be reduced and improve coefficient stability. More samples than parameters are needed for the model to be stable. Lastly, larger dataset have statistical reliability and can capture all the real-world variability. Models used in this project could be improved with more dataset records in the future. </li>
    <li>There was a class imbalance in the dataset used in the model (80% non-smoker / 20% smoker). A balanced dataset means both classes appear in roughly equal proportions (i.e., 50/50 or 60/40). Having balanced classes in logistic regression is crucial for the model to learn both classes fairly, produce unbiased coefficients, and make reliable predictions, especially for the minority class that often carries the most significance.</li>
</ul>






























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































    


